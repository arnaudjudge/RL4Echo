work_dir: ${hydra:runtime.cwd}
run_name: default

defaults:
  - logger: tensorboard
  - model: ppo
  - datamodule: ???
  - callbacks: default
  - experiment: ppo_es_ed
  - _self_
#  - override /reward@model.reward: rewardunet_multihead
#  - override /reward@model.reward: pixelwise_accuracy


#datamodule:
#  class_label: 1
#model:
#  class_label: 1

model:
  reward:
    state_dict_path: /data/rl_logs/1_entropy/3/rewardnet.ckpt
#    temp_factor: 1 #4.1778
  actor:
    actor:
      pretrain_ckpt: /data/rl_logs/1_entropy/0/actor.ckpt #/data/rl_logs/1_entropy/4/actor.ckpt #./../UDAS/icardio_real_1/model_refined.ckpt

seed: 1

logger:
  save_dir: /data/rl_logs/test/
  name: "v2"

#ckpt_path:  /data/rl_logs//run_1_batchsize_increase/PPO_RewardUnet_es_ed/version_3/checkpoints/epoch=1-step=5920.ckpt # /data/rl_logs//run_6_batchsize_increase/PPO_RewardUnet_es_ed/version_3/checkpoints/epoch=0-step=2960.ckpt

trainer:
  _target_: lightning.pytorch.Trainer
  log_every_n_steps: 1
  accelerator: gpu
  devices: 1
#  num_sanity_val_steps: 0 # skip sanity check (convenient for debugging directly in train step)
